{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Self Learning Tutorial\n",
    "Vital signs: \n",
    "systolic blood pressure (SBP),\n",
    "mean arterial pressure (MAP), \n",
    "respiratory rate (RR), \n",
    "oxygen saturation (PulOx), \n",
    "heart rate (HR), \n",
    "temperature (Temp)\n",
    "\n",
    "Lab results: \n",
    "white blood cell count (WBC), \n",
    "bilirubin (Bili), \n",
    "blood urea nitrogen (BUN), \n",
    "lactate (Lac), \n",
    "creatinine (Creat), \n",
    "platelet count (Plat), \n",
    "neutrophils (Bands)\n",
    "\n",
    "Intervention: fraction of inspired Oxygen (FiO2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CHARTEVENTS table in MIMIC III contains patient vitals. Key concepts related to vital signs can be identified by itemid. For example:​\n",
    "\n",
    "Systolic Blood Pressure (SBP): itemid = 220050 (ART BP Systolic)​\n",
    "\n",
    "Mean Arterial Pressure (MAP): itemid = 220052 (ART Mean BP)​\n",
    "\n",
    "Respiratory Rate (RR): itemid = 220210 (Respiratory Rate)​\n",
    "\n",
    "Oxygen Saturation (PulOx): itemid = 220277 (O2 saturation pulseoxymetry)​\n",
    "\n",
    "Heart Rate (HR): itemid = 220045 (Heart Rate)​\n",
    "\n",
    "Temperature (Temp): itemid = 223761 (Temperature Fahrenheit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from the CHARTEVENTS table in the MIMIC-III database\n",
    "dataset_labevents = 'mimic/LABEVENTS.csv'\n",
    "dataset_chartevents = 'mimic/CHARTEVENTS.csv'\n",
    "dataset_labitems = 'mimic/D_LABITEMS.csv'\n",
    "#'mimic/DIAGNOSES_ICD.csv'\n",
    "#dataset_path3 = 'mimic/D_ICD_DIAGNOSES.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id  itemid                             label   abbreviation  \\\n",
      "11498   12712  220045                        Heart Rate             HR   \n",
      "11502   12716  220050  Arterial Blood Pressure systolic           ABPs   \n",
      "11504   12718  220052      Arterial Blood Pressure mean           ABPm   \n",
      "11524   12738  220210                  Respiratory Rate             RR   \n",
      "12355   12746  220277       O2 saturation pulseoxymetry           SpO2   \n",
      "12366   12757  223761            Temperature Fahrenheit  Temperature F   \n",
      "\n",
      "         dbsource      linksto             category  unitname param_type  \\\n",
      "11498  metavision  chartevents  Routine Vital Signs       bpm    Numeric   \n",
      "11502  metavision  chartevents  Routine Vital Signs      mmHg    Numeric   \n",
      "11504  metavision  chartevents  Routine Vital Signs      mmHg    Numeric   \n",
      "11524  metavision  chartevents          Respiratory  insp/min    Numeric   \n",
      "12355  metavision  chartevents          Respiratory         %    Numeric   \n",
      "12366  metavision  chartevents  Routine Vital Signs        ?F    Numeric   \n",
      "\n",
      "       conceptid  \n",
      "11498        NaN  \n",
      "11502        NaN  \n",
      "11504        NaN  \n",
      "11524        NaN  \n",
      "12355        NaN  \n",
      "12366        NaN  \n"
     ]
    }
   ],
   "source": [
    "# show the chart events\n",
    "import pandas as pd\n",
    "dataset_chartitems = 'mimic/D_ITEMS.csv'\n",
    "df_chartitems = pd.read_csv(dataset_chartitems)\n",
    "\n",
    "df_chartitems.columns = df_chartitems.columns.str.lower()\n",
    "result = df_chartitems.loc[df_chartitems['itemid'].isin([ 220050, 220052, 220210, 220277, 220045, 223761])]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Results:​\n",
    "\n",
    "The LABEVENTS table stores the laboratory measurements.​\n",
    "\n",
    "White Blood Cell Count (WBC): itemid = 51300 (White Blood Cells)​\n",
    "\n",
    "Bilirubin (Bili): itemid = 50885 (Bilirubin, Total)​\n",
    "\n",
    "Blood Urea Nitrogen (BUN): itemid = 51006 (Urea Nitrogen)​\n",
    "\n",
    "Lactate (Lac): itemid = 50813 (Lactate)​\n",
    "\n",
    "Creatinine (Creat): itemid = 50912 (Creatinine)​\n",
    "\n",
    "Platelet Count (Plat): itemid = 51265 (Platelets)​\n",
    "\n",
    "Neutrophils (Bands): itemid = 51146 (Bands - Neutrophils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from the LabEvents table in the MIMIC-III database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     row_id  itemid             label  fluid    category loinc_code\n",
      "140      14   50813           Lactate  Blood   Blood Gas    32693-4\n",
      "212      86   50885  Bilirubin, Total  Blood   Chemistry     1975-2\n",
      "239     113   50912        Creatinine  Blood   Chemistry     2160-0\n",
      "332     206   51006     Urea Nitrogen  Blood   Chemistry     3094-0\n",
      "472     346   51146         Basophils  Blood  Hematology      704-7\n",
      "591     465   51265    Platelet Count  Blood  Hematology      777-3\n",
      "626     500   51300         WBC Count  Blood  Hematology    26464-8\n"
     ]
    }
   ],
   "source": [
    "# show the lab items\n",
    "import pandas as pd\n",
    "df_labitems = pd.read_csv(dataset_labitems)\n",
    "\n",
    "df_labitems.columns = df_labitems.columns.str.lower()\n",
    "result = df_labitems.loc[df_labitems['itemid'].isin([51300, 50885, 51006, 50813, 50912, 51265, 51146])]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interventions:​\n",
    "\n",
    "The INPUTEVENTS_MV or INPUTEVENTS_CV table can be used to find oxygen administration values.​\n",
    "\n",
    "Fraction of Inspired Oxygen (FiO2): itemid = 223835 (FiO2 in the CHARTEVENTS table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data inputevents_mv table in the MIMIC-III database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id  itemid                 label abbreviation    dbsource  \\\n",
      "12413   12804  223835  Inspired O2 Fraction         FiO2  metavision   \n",
      "\n",
      "           linksto     category unitname param_type  conceptid  \n",
      "12413  chartevents  Respiratory      NaN    Numeric        NaN  \n"
     ]
    }
   ],
   "source": [
    "# show FiO2 item\n",
    "result = df_chartitems.loc[df_chartitems['itemid'].isin([223835])]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICD-9 Codes for Sepsis-Related Identification:\n",
    "To identify patients with sepsis and septic shock, ICD-9 codes are used in the DIAGNOSES_ICD table. Relevant codes include:\n",
    "\n",
    "Sepsis: 99591 (Sepsis), 99592 (Severe Sepsis)\n",
    "Septic Shock: 78552 (Septic Shock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       row_id icd9_code    short_title     long_title\n",
      "10304   11403     99591         Sepsis         Sepsis\n",
      "10305   11404     99592  Severe sepsis  Severe sepsis\n",
      "13142   12991     78552   Septic shock   Septic shock\n"
     ]
    }
   ],
   "source": [
    "# Get data from DIAGNOSES_ICD table in the MIMIC-III database using ICD-9 codes for sepsis related identification\n",
    "dataset_icd9_code = 'mimic/D_ICD_DIAGNOSES.csv'\n",
    "df_icd9_code = pd.read_csv(dataset_icd9_code)\n",
    "\n",
    "df_icd9_code.columns = df_icd9_code.columns.str.lower()\n",
    "result = df_icd9_code.loc[df_icd9_code['icd9_code'].isin(['99591', '99592', '78552'])]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load chartevents in chunks to avoid overloading memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROW_ID  SUBJECT_ID  HADM_ID  SEQ_NUM ICD9_CODE\n",
      "0    1297         109   172335      1.0     40301\n",
      "1    1298         109   172335      2.0       486\n",
      "2    1299         109   172335      3.0     58281\n",
      "3    1300         109   172335      4.0      5855\n",
      "4    1301         109   172335      5.0      4254\n"
     ]
    }
   ],
   "source": [
    "dataset_diagnoses = 'mimic/DIAGNOSES_ICD.csv'\n",
    "diagnoses_icd = pd.read_csv(dataset_diagnoses)\n",
    "print(diagnoses_icd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the relevant itemids for vital signs and FiO2\n",
    "vital_signs_itemids = [220050, 220052, 220210, 220277, 220045, 223761]  # SBP, MAP, RR, PulOx, HR, Temp\n",
    "fio2_itemid = [223835]  # FiO2\n",
    "relevant_itemids = vital_signs_itemids + fio2_itemid\n",
    "\n",
    "# Load diagnoses to identify sepsis patients\n",
    "diagnoses_icd = pd.read_csv(dataset_diagnoses, usecols=['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE'])\n",
    "sepsis_icd9_codes = ['99591', '99592', '78552']  # Sepsis-related ICD-9 codes\n",
    "sepsis_patients = diagnoses_icd #[diagnoses_icd['ICD9_CODE'].isin(sepsis_icd9_codes)]\n",
    "\n",
    "# Create a set of hadm_ids for sepsis patients for fast lookup\n",
    "sepsis_hadm_ids = set(sepsis_patients['HADM_ID'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_results_itemids = [51300, 50885, 51006, 50813, 50912, 51265, 51146] # WBC, HCO3, PLATELET, PH, GLUCOSE, BUN, CREATININE\n",
    "labevents = pd.read_csv(dataset_labevents, usecols=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME', 'ITEMID', 'VALUENUM'])\n",
    "labevents = labevents[labevents['ITEMID'].isin(lab_results_itemids)]\n",
    "lab_hadm_ids = set(labevents['HADM_ID'].unique())\n",
    "# add lab hadm_ids to sepsis_hadm_ids\n",
    "sepsis_hadm_ids.update(lab_hadm_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store filtered data\n",
    "filtered_chartevents = []\n",
    "\n",
    "# Define the chunk size (number of rows to read at a time)\n",
    "chunk_size = 100000  # 100k rows per chunk\n",
    "\n",
    "# Read CHARTEVENTS in chunks\n",
    "chartevents_chunks = pd.read_csv(dataset_chartevents, usecols=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME', 'ITEMID', 'VALUENUM'], chunksize=chunk_size)\n",
    "\n",
    "# Process each chunk\n",
    "for chunk in chartevents_chunks:\n",
    "    # Filter the chunk based on relevant itemid and sepsis patients' hadm_id\n",
    "    filtered_chunk = chunk[(chunk['ITEMID'].isin(relevant_itemids)) & (chunk['HADM_ID'].isin(sepsis_hadm_ids))]\n",
    "    \n",
    "    # Append the filtered data to the list\n",
    "    filtered_chartevents.append(filtered_chunk)\n",
    "\n",
    "# Concatenate all the filtered chunks into a single DataFrame\n",
    "filtered_chartevents_df = pd.concat(filtered_chartevents, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the filtered data to a CSV file\n",
    "filtered_chartevents.to_csv('filtered_chartevents.csv', index=False)\n",
    "# write the labevents data to a CSV file\n",
    "filtered_labevents.to_csv('filtered_labevents.csv', index=False)\n",
    "#writing the sepsis patients data to a CSV file\n",
    "sepsis_patients.to_csv('sepsis_patients.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the filtered data csv files and merge them using subject id and hadm id\n",
    "import pandas as pd\n",
    "filtered_chartevents = pd.read_csv('filtered_chartevents.csv')\n",
    "filtered_labevents = pd.read_csv('filtered_labevents.csv')\n",
    "sepsis_patients = pd.read_csv('sepsis_patients.csv')\n",
    "# merge the filtered chartevents, sepsis patients and labevents data\n",
    "#merge_charts = pd.merge(filtered_chartevents,sepsis_patients, on=['SUBJECT_ID', 'HADM_ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SUBJECT_ID  HADM_ID  ITEMID            CHARTTIME  VALUENUM\n",
      "0          36   165660  223835  2134-05-12 12:00:00     100.0\n",
      "1          36   165660  220045  2134-05-12 13:00:00      86.0\n",
      "2          36   165660  220210  2134-05-12 13:00:00      21.0\n",
      "3          36   165660  220277  2134-05-12 13:00:00      93.0\n",
      "4          36   165660  220045  2134-05-12 14:00:00      85.0\n",
      "   SUBJECT_ID  HADM_ID  ITEMID            CHARTTIME  VALUENUM\n",
      "0           3      NaN   50813  2101-10-12 18:17:00       1.8\n",
      "1           3      NaN   50912  2101-10-13 03:00:00       1.7\n",
      "2           3      NaN   51006  2101-10-13 03:00:00      33.0\n",
      "3           3      NaN   50912  2101-10-13 15:47:00       1.5\n",
      "4           3      NaN   51006  2101-10-13 15:47:00      32.0\n",
      "   SUBJECT_ID  HADM_ID ICD9_CODE\n",
      "0         109   172335     40301\n",
      "1         109   172335       486\n",
      "2         109   172335     58281\n",
      "3         109   172335      5855\n",
      "4         109   172335      4254\n"
     ]
    }
   ],
   "source": [
    "print(filtered_chartevents.head())\n",
    "print(filtered_labevents.head())\n",
    "print(sepsis_patients.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJECT_ID         0\n",
      "HADM_ID       669625\n",
      "ITEMID             0\n",
      "CHARTTIME          0\n",
      "VALUENUM         556\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# find empty values in the filtered labeevents data\n",
    "print(filtered_labevents.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2967466, 5)\n"
     ]
    }
   ],
   "source": [
    "# find size of filtered labevents data\n",
    "print(filtered_labevents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with empty values in the filtered labevents data\n",
    "filtered_labevents = filtered_labevents.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJECT_ID    0\n",
      "HADM_ID       0\n",
      "ITEMID        0\n",
      "CHARTTIME     0\n",
      "VALUENUM      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# find empty values in the filtered chartevents data\n",
    "print(filtered_chartevents.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJECT_ID     0\n",
      "HADM_ID        0\n",
      "ICD9_CODE     47\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# find empty values in the sepsis patients data\n",
    "print(sepsis_patients.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with empty values in the sepsis patients data\n",
    "sepsis_patients = sepsis_patients.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SUBJECT_ID',    'HADM_ID',  'CHARTTIME',       220045,       220050,\n",
       "             220052,       220210,       220277,       223761,       223835],\n",
       "      dtype='object', name='ITEMID')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pivot filtered_chartevents data by itemid\n",
    "filtered_chartevents_pivot = filtered_chartevents.pivot_table(index=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME'], columns='ITEMID', values='VALUENUM', aggfunc='first').reset_index()\n",
    "filtered_chartevents_pivot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the filtered_chartevents_pivot data to a CSV file\n",
    "filtered_chartevents_pivot.to_csv('filtered_chartevents_pivot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot filtered_labevents data by itemid\n",
    "filtered_labevents_pivot = filtered_labevents.pivot_table(index=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME'], columns='ITEMID', values='VALUENUM', aggfunc='first').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SUBJECT_ID',    'HADM_ID',  'CHARTTIME',        50813,        50885,\n",
       "              50912,        51006,        51146,        51265,        51300],\n",
       "      dtype='object', name='ITEMID')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_labevents_pivot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the filtered_labevents_pivot data to a CSV file\n",
    "filtered_labevents_pivot.to_csv('filtered_labevents_pivot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(905872, 10)\n",
      "(3036046, 10)\n"
     ]
    }
   ],
   "source": [
    "# count filtered_labevents_pivot data and filtered_chartevents_pivot data\n",
    "print(filtered_labevents_pivot.shape)\n",
    "print(filtered_chartevents_pivot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the filtered_labevents_pivot and filtered_chartevents_pivot data by subject id and hadm id\n",
    "combined_data = pd.merge(filtered_labevents_pivot, filtered_chartevents_pivot, on=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME'], how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the combined data to a CSV file\n",
    "combined_data.to_csv('combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITEMID\n",
      "SUBJECT_ID          0\n",
      "HADM_ID             0\n",
      "CHARTTIME           0\n",
      "50813         3753181\n",
      "50885         3757307\n",
      "50912         3311643\n",
      "51006         3313886\n",
      "51146         3821674\n",
      "51265         3332301\n",
      "51300         3930540\n",
      "220045        1168913\n",
      "220050        2781350\n",
      "220052        2774965\n",
      "220210        1194033\n",
      "220277        1259322\n",
      "223761        3408995\n",
      "223835        3373077\n",
      "SEPSIS              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count the empty values in the combined data\n",
    "print(combined_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd9_code_sepsis = ['99591', '99592', '78552']\n",
    "\n",
    "# get unique hadm_ids for sepsis patients\n",
    "sepsis_hadm_ids = sepsis_patients[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n",
    "\n",
    "# update combined_data with sepsis label\n",
    "combined_data['SEPSIS'] = combined_data[['SUBJECT_ID', 'HADM_ID']].apply(tuple, axis=1).isin(sepsis_hadm_ids.apply(tuple, axis=1)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEPSIS\n",
      "1    3930988\n",
      "0        150\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count the number of sepsis patients\n",
    "print(combined_data['SEPSIS'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the combined data to a CSV file\n",
    "combined_data.to_csv('combined_data_with_labels.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SUBJECT_ID',    'HADM_ID',  'CHARTTIME',        50813,        50885,\n",
       "              50912,        51006,        51146,        51265,        51300,\n",
       "             220045,       220050,       220052,       220210,       220277,\n",
       "             223761,       223835,     'SEPSIS'],\n",
       "      dtype='object', name='ITEMID')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert combined data to training data for below model\n",
    "combined_data.columns.size\n",
    "combined_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47377/867793550.py:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  combined_data.fillna(method='ffill', inplace=True)  # Forward-fill missing data\n"
     ]
    }
   ],
   "source": [
    "# load combined data with labels\n",
    "combined_data = pd.read_csv('combined_data_with_labels.csv')\n",
    "combined_data.fillna(method='ffill', inplace=True)  # Forward-fill missing data\n",
    "combined_data.fillna(0, inplace=True)  # Fill remaining NaNs with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBJECT_ID    0\n",
      "HADM_ID       0\n",
      "CHARTTIME     0\n",
      "50813         0\n",
      "50885         0\n",
      "50912         0\n",
      "51006         0\n",
      "51146         0\n",
      "51265         0\n",
      "51300         0\n",
      "220045        0\n",
      "220050        0\n",
      "220052        0\n",
      "220210        0\n",
      "220277        0\n",
      "223761        0\n",
      "223835        0\n",
      "SEPSIS        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# find count of empty values in the combined data\n",
    "print(combined_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "grouped = combined_data.groupby(['SUBJECT_ID', 'HADM_ID'])\n",
    "\n",
    "# Prepare the sequences for the LSTM model\n",
    "X_sequences = []\n",
    "y_labels = []\n",
    "\n",
    "for (subject_id, hadm_id), group in grouped:\n",
    "    # Drop 'SUBJECT_ID', 'HADM_ID', 'CHARTTIME' to keep only the feature columns (ITEMIDs)\n",
    "    X_seq = torch.tensor(group.drop(columns=['SUBJECT_ID', 'HADM_ID', 'CHARTTIME', 'SEPSIS']).values, dtype=torch.float32)\n",
    "    \n",
    "    # Append the sequence to the list\n",
    "    X_sequences.append(X_seq)\n",
    "    \n",
    "    # Get the label for this admission (assuming 'SEPSIS' is the same for the entire group)\n",
    "    y_label = group['SEPSIS'].values[0]  # Assumes SEPSIS is consistent within an HADM_ID\n",
    "    y_labels.append(torch.tensor(y_label, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 14])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_sequences[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_sequences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_sequences))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_labels))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(X_sequences[0].shape)\n",
    "\n",
    "print(len(X_sequences))\n",
    "print(len(y_labels))\n",
    "print(y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = torch.tensor(y_labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 44020\n",
      "Min sequence length: 1\n",
      "Average sequence length: 67.64062768849581\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths = [len(seq) for seq in X_sequences]\n",
    "print(f\"Max sequence length: {max(sequence_lengths)}\")\n",
    "print(f\"Min sequence length: {min(sequence_lengths)}\")\n",
    "print(f\"Average sequence length: {sum(sequence_lengths) / len(sequence_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "max_seq_length = 100  # Adjust as needed\n",
    "\n",
    "# Truncate sequences to the maximum length\n",
    "X_sequences_truncated = [seq[:max_seq_length] for seq in X_sequences]\n",
    "\n",
    "# Pad the truncated sequences\n",
    "X_padded = pad_sequence(X_sequences_truncated, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LSTM model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Decode the last hidden state\n",
    "        out = self.fc(out[:, -1, :])  # Get the output from the last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters\n",
    "input_size = 14  # Example: 14 original + 6 additional features\n",
    "hidden_size = 64\n",
    "output_size = 1  # Binary classification (septic shock or not)\n",
    "num_layers = 2\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(14, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create DataLoader for training and testing\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMModel(input_size, hidden_size, output_size, num_layers)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy with logits\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 0.0040\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        outputs = outputs.squeeze(dim=1)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9995\n",
      "Precision: 0.9995\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9997\n",
      "AUC: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy()  # Use sigmoid to get probabilities\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(y_batch.numpy())\n",
    "\n",
    "y_pred = [1 if p > 0.5 else 0 for p in y_pred]  # Convert probabilities to binary labels\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-rl-ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
